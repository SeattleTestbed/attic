\section{Architecture}
\label{sec-architecture}

To use Seattle, the instructor creates an account on our website and obtains
an installer.   The machines that run the installer (such as computers in 
the universities computer lab) donate resources that are credited to the 
instructor.   The instructor can then obtain resources on machines around the 
world.   As of December 1st, 2008 the current policy is that for each donation,
the instructor receives resources on ten other computers.   However, the 
instructor can delegate those resources either directly to students or
to TAs who do more fine-grained delegation.   Students and TAs download
a toolkit and then experiment with their resources.

Seattle's architecture is comprised of several components. At the
lowest level the \emph{sandbox} component guarantees security and
resource control for an individual program. Programs are written to
the Seattle API in a subset of the Python programming language. This
API provides portable access to low level operations (like opening
files and sending messages). At a higher layer, the \emph{node
  manager} determines which sandboxed programs get to run on the local
computer. A public key infrastructure is used to authorize control
over sandboxed programs. Lastly, the \emph{experiment manager} lets
students control their program instances across computers.

\subsection{Seattle API}
\label{sec-API}

Seattle provides a programming API for low-level operations (like
writing to files or sending network messages) and maintains program
portability using an abstraction layer. Platform specific code below
this abstraction layer handles non-portable operations enabling
unmodified programs to run on a wide variety of
platforms. % This also allows the majority of the sandbox to be reused
%across different architectures.


The API consists of five categories: file, network, timer, locking,
and miscellaneous. The file API calls enable limited access to the
local computer's persistent storage (interacting only with files in a 
single directory). 
%the calls to
%create, delete, open, read and write sandboxed files, as well as calls
%to list directory contents in the sandbox. 
The network API calls
provide the local IP address, perform a DNS lookup, enable sending and
receiving of UDP messages, and managing of and communicating over TCP
connections. The timer API calls enable the programmer to put the
current thread to sleep and to schedule functions to be called at
later times. For example, the programmer can register an event to
periodically send a heartbeat message to another computer. The locking
functions allow the programmer to handle concurrency in their program
(as common state may be accessed and modified by multiple threads at
the same time). The miscellaneous API calls allow the programmer to
exit the program, to generate random numbers, and to provide the
amount of time the program has been running.

% initiation/termination of and listening for TCP connections

\subsection{The Sandbox}
\label{sec-repy}

The sandbox's primary goal is to securely execute user code. There are
two aspects to this --- preventing insecure actions and limiting
resource consumption. To prevent insecure actions the sandbox hooks into 
the Python parser and reads the program's parse tree. Only actions that
the sandbox can verify as safe may execute.

To control resource consumption on the host the sandbox interposes on
all API calls made by a program. The sandbox monitors the overall use
of resources like CPU, memory, and disk space to ensure the program
does not exceed its bounds. Each API call that uses a monitored
resource is evaluated before being granted or denied the resource. The
sandbox also restricts the rate at which API calls are performed.

The restrictions and resource limits of the sandbox are configurable and
may restrict different programs in different ways. For example, one 
program may only be
allowed to receive UDP packets on port 11111, while another program
may be restricted to receiving UDP packets on port 22222. This enables
multiple sandboxes on the same computer to host programs controlled by
different users.

%We provide an interface where the program can execute unsafe operations (such 
%as open files, send network traffic, etc.).   Our sandbox verifies that the 
%program is performing a safe operation (like opening a file in the program's 
%sandbox) instead of a malicious action (like opening the user's credit card 
%information).   This is done by fine grained checks on the arguments passed 
%to individual calls.   Once again we err on the side of caution (for example, 
%restricting the names of files the program can open) to prevent the program 
%from escaping the sandbox.
%
%However, not every safe action should be allowed by every program.   It may 
%be that two programs would like to run on the same system and each program 
%has been allocated its own port.   The programs should not be able to use 
%the other program's port or other ports on the system.   To prevent this 
%repy has per-program fine-grained restrictions that control the use of 
%calls.   Program A can be restricted to port 12345 while Program B is 
%restricted to port 54321.   Similarly, a Program C that does not need 
%to write files to the disk can be prevented from using file operations all 
%together!   This meets our design goal of a multi-user programming environment
%since students are isolated from each other.
%
%
%\subsubsection{Resource Limits}
%\label{sec-resourcelimits}
%
%Another important type of isolation is resource isolation.   One program that 
%runs on a computer should not be able to consume enough resources to 
%impact the execution of other programs or the host computer.   To this end, 
%repy limits resource consumption through several mechanisms.   For 
%resources that renew themselves over time (like CPU, network send rate, file 
%write rate, etc.) the program is paused if it tries to over use the resource 
%so that the performance of the system does not suffer.   After a suitable 
%amount of time, the program is restarted.    If the resource is not renewable 
%(like the number of open files, memory use, disk used, etc.) then either the 
%function raises an exception or the program is killed.   This meets our
%design goal of preventing the 
%program from negatively impacting the performance of other programs running 
%on the same system.
%


\subsection{Node Manager}
\label{sec-nodemanager}
While useful in itself, the sandbox is part of a larger ecosystem.
The sandbox isolates a specific running program on a host computer,
but does not address how that program is started, which programs are
run, and who has permission to run a program. Such functionality is
provided by the node manager, which manages sandboxed running programs
as part of what we call \emph{vessels}. The node manager stores
information about the vessels it controls and allows vessels to be
started, stopped, combined, split, and changed.


\subsubsection{Vessels}

A vessel is a controlled environment for running code (implemented
using the Seattle sandbox).  Intuitively, a vessel includes the
program's sandbox and the node manager state (such as the resources
and restrictions assigned to the program).  Vessels have well defined
boundaries that prevent them from interfering with one another (for
example, different vessels have their own disjoint set of network
ports).  Each vessel has associated with it a restrictions file, a
stop file, and a log.  The restrictions file lists what the vessel can
and cannot do (such as the network ports that can be used) along with
the amount of each resource the vessel may use.  The stop file enables
the node manager to stop the vessel (by creating a file with that
name). The log is a circular buffer written by the vessel to
communicate useful information to the vessel owner. The log helps the
programmer to diagnose failures and to capture program state for
off-line analysis.

% (who has the initial set of resources since they installed
% the software)
A common scenario is for a student to obtain a vessel from their
instructor. The student then decides the program they want to run in
their vessel. To do this, she directs the experiment manager to
install the program on a node. The experiment manager uploads the
program to the student's vessel (along with any data files) and
executes the program in the vessel. The student also can easily perform
this action on groups of vessels spread across many different nodes.
The student can then monitor the status of her program by looking at a 
status indicator provided by querying the node manager (coarse-grained 
monitoring) or by downloading information 
about the program from its circular log buffer (fine-grained monitoring).  
The user can also stop the vessel while retaining all of the state so that 
she can examine data files and logs.   

In a more complex example an instructor splits a single vessel on a
node into multiple vessels, and
assigns each vessel to a student
in the class. Vessels may also be combined for flexibility. For example,
the students may be allowed to work in groups. Once the groups are
formed, some of the students may decide to combine their vessels so as
to get more resources in a single vessel.

% ib: do we need to describe offcut resources? seems too detailed.
% jac: commented out
%
%When a vessel is split, the quantity of resources isn't exactly
%divided between the two new vessels because of inherent overhead
%inherent in managing a vessel. To capture this effect, there is a
%resource specification for offcut resources.  Whenever a vessel is
%split, an amount of resources equal to the offcut resources is lost.
%Whenever two vessels are joined, the vessels gain resources equal to
%the offcut resources.

\subsection{Locating Seattle Nodes}
\label{sec-locationservice}

It is important to note that there is nothing parallel or distributed
about the node manager. The node manager only manages the vessels on
the local system. To facilitate global location of resources, the node
manager inserts a key/value pair into two different public DHTs
(OpenDHT~\cite{Rhea_SIGCOMM_2005}) every five minutes. The inserted key is
the owner's public key and the value is the local computer's IP
address. This allows a user to lookup their public key to find
the nodes with vessels they control without needing to keep track of
these nodes on their own.

\subsection{Experiment Manager}
\label{sec-experimentmanager}

The experiment manager is the main tool in the toolkit that students use
to interact with Seattle. The experiment manager
transparently handles discovery of vessels the user controls by
querying the DHT, and communicates with remote node managers to perform
actions on the user's behalf.

The experiment manager provides the user with a shell interface
(similar to PLuSH~\cite{Albrecht_LISA_2007}) in which the user can
issue commands. For example, users can install software in their
vessels -- the experiment manager uploads a program into the vessels
the user specifies. Users can also start and stop vessels, or report
on the status of a vessel. A vessel's status can be fresh (has never
run a program), started (is running a program), stopped (was requested
to stop), or terminated (terminated due to a normal exit or an unhandled 
program exception). When a vessel has failed
(perhaps due to a bug in the student's code), exception information
with a stack trace of the fault is logged. The student can use
the experiment manager to find the failed program instances to inspect
their logs or to see an exception's stack trace.


